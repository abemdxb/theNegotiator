{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl_eFVsjK3cA"
      },
      "source": [
        "# **The Negotiator: A Testbed for evaluating RAG strategies**\n",
        "Hello! Welcome to Abraham Alappat's personal project- a QA system to help people conduct negotiations for sales and procurement! In this and other notebooks, we will use a test-bed RAG QA product to test various strategies to optimize rag.\n",
        "\n",
        "In the longer run we will convert this test-bed into a productized ML pipeline that will be further iterated.\n",
        "\n",
        "**Goal for this particular notebook:** Test which chunking strategy helps optimize ML performance, latency and throughput for a set of standardized 50 questions.  \n",
        "\n",
        "**The Test-bed Product:** A Q/A tool that refers to best practices negotiation tactics for any given sales or procurement situation and does not contain contradictory information which is available in web-scale data. This notebook represents the MVP exploration of such a product and will be iterated on to test various aspects of .\n",
        "\n",
        "\n",
        "---------------------------------\n",
        "Note that this notebook follows the best tradition of software... re-use of code! I wanted a RAG based Q&A tool so adapted the Arize's Tutorials on Pinecone Search and Retrieval, Deep Learning's Prompt Engineering Course, and tracing.\n",
        "\n",
        "Things out of scope given the time for this project:\n",
        "1. Optimizing K, model temperature, model type, vector DB, system prompt, etc.\n",
        "\n",
        "*Code sourced from elsewhere will be marked as \"sourced from [source]\" in code comments*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bbAnVUIusb"
      },
      "source": [
        "# Step 1: Install Libraries and Dependences + Obtain Pinecone and OpenAI details\n",
        "**Background:** Here are we installing the usual libraries, packages and dependencies + collecting info to allow for downstream functions.\n",
        "\n",
        "**Intructions:** Run all cells and enter your OpenAI API Key, as well as your Pinecone API Key, environment and Index name to enable the full notebook to run! The Pinecone index should be set up in the same way as the Arize RAG tutorial.\n",
        "\n",
        "**1.1 Installations and Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cython in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (0.29.36)\n",
            "Requirement already satisfied: numpy in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (1.25.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install cython numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O8yi7Olm2Wu",
        "outputId": "4a89b550-3a98-4e21-e0d2-a461e9b86681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (0.0.350)\n",
            "Requirement already satisfied: openai==0.28.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (0.28.1)\n",
            "Requirement already satisfied: pinecone-client in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (1.0.0)\n",
            "Requirement already satisfied: pypdf in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (3.17.2)\n",
            "Requirement already satisfied: cohere in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (4.38)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (0.5.2)\n",
            "Requirement already satisfied: arize-phoenix in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (1.9.0)\n",
            "Requirement already satisfied: unstructured in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (0.11.2)\n",
            "Requirement already satisfied: fastparquet in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (2023.10.1)\n",
            "Requirement already satisfied: pyarrow in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (14.0.1)\n",
            "Requirement already satisfied: GitPython in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (3.1.40)\n",
            "Requirement already satisfied: tqdm in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (4.66.1)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from openai==0.28.1) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (0.0.3)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (0.1.0)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (0.0.70)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (1.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (2.5.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: loguru>=0.5.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pinecone-client) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pinecone-client) (4.8.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pinecone-client) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pinecone-client) (2.0.3)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from cohere) (1.9.1)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
            "Requirement already satisfied: ddsketch in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (2.0.4)\n",
            "Requirement already satisfied: hdbscan<1.0.0,>=0.8.33 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (0.8.33)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (3.1.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (2.0.3)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.20 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (4.25.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (5.9.0)\n",
            "Requirement already satisfied: scikit-learn<1.3.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (1.2.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (1.11.4)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (2.4.0)\n",
            "Requirement already satisfied: starlette in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (0.33.0)\n",
            "Requirement already satisfied: strawberry-graphql==0.208.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (0.208.2)\n",
            "Requirement already satisfied: umap-learn in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (0.5.5)\n",
            "Requirement already satisfied: uvicorn in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (0.24.0.post1)\n",
            "Requirement already satisfied: wrapt in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from arize-phoenix) (1.16.0)\n",
            "Requirement already satisfied: graphql-core<3.3.0,>=3.2.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from strawberry-graphql==0.208.2->arize-phoenix) (3.2.3)\n",
            "Requirement already satisfied: chardet in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (4.12.2)\n",
            "Requirement already satisfied: emoji in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (2.9.0)\n",
            "Requirement already satisfied: python-iso639 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (2023.12.11)\n",
            "Requirement already satisfied: langdetect in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from unstructured) (3.5.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from fastparquet) (2.7.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from fastparquet) (2023.12.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from fastparquet) (23.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from GitPython) (4.0.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from aiohttp->openai==0.28.1) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.1)\n",
            "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from hdbscan<1.0.0,>=0.8.33->arize-phoenix) (0.29.36)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from hdbscan<1.0.0,>=0.8.33->arize-phoenix) (1.3.2)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain) (4.1.0)\n",
            "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pandas->arize-phoenix) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pandas->arize-phoenix) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from requests>=2.20->openai==0.28.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from requests>=2.20->openai==0.28.1) (2023.5.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from scikit-learn<1.3.0->arize-phoenix) (3.2.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from jinja2->arize-phoenix) (2.1.3)\n",
            "Requirement already satisfied: click in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from umap-learn->arize-phoenix) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from umap-learn->arize-phoenix) (0.5.11)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from uvicorn->arize-phoenix) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from numba>=0.51.2->umap-learn->arize-phoenix) (0.41.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval Tutorial + Langchain Tutorial + DeepLearning Tutorials\n",
        "\n",
        "%pip install langchain openai==0.28.1 pinecone-client python-dotenv pypdf cohere tiktoken arize-phoenix unstructured fastparquet pyarrow GitPython tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Base tools\n",
        "import os\n",
        "import textwrap\n",
        "from getpass import getpass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from typing_extensions import dataclass_transform\n",
        "from pandas.io import parquet\n",
        "# from google.colab import drive\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from contextlib import contextmanager\n",
        "from git import Repo\n",
        "\n",
        "#OpenAI and Langchain\n",
        "import openai\n",
        "import langchain\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain import hub\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "#Observational Tools and vector database\n",
        "import phoenix as px\n",
        "from phoenix.trace.langchain import OpenInferenceTracer, LangChainInstrumentor\n",
        "import pinecone\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (8.1.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipywidgets) (8.16.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipywidgets) (5.12.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.9 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
            "Requirement already satisfied: backcall in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.8)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Package                       VersionNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "----------------------------- ------------\n",
            "aiohttp                       3.9.1\n",
            "aiosignal                     1.3.1\n",
            "annotated-types               0.6.0\n",
            "anyio                         4.1.0\n",
            "arize-phoenix                 1.9.0\n",
            "asttokens                     2.4.0\n",
            "async-timeout                 4.0.3\n",
            "attrs                         23.1.0\n",
            "backcall                      0.2.0\n",
            "backoff                       2.2.1\n",
            "backports.functools-lru-cache 1.6.5\n",
            "beautifulsoup4                4.12.2\n",
            "certifi                       2023.5.7\n",
            "chardet                       5.2.0\n",
            "charset-normalizer            3.2.0\n",
            "click                         8.1.7\n",
            "cohere                        4.38\n",
            "colorama                      0.4.6\n",
            "comm                          0.1.4\n",
            "cramjam                       2.7.0\n",
            "Cython                        0.29.36\n",
            "dataclasses-json              0.6.3\n",
            "ddsketch                      2.0.4\n",
            "debugpy                       1.6.7\n",
            "decorator                     5.1.1\n",
            "dnspython                     2.4.2\n",
            "emoji                         2.9.0\n",
            "exceptiongroup                1.1.3\n",
            "executing                     1.2.0\n",
            "fastavro                      1.9.1\n",
            "fastparquet                   2023.10.1\n",
            "filetype                      1.2.0\n",
            "frozenlist                    1.4.0\n",
            "fsspec                        2023.12.2\n",
            "gitdb                         4.0.11\n",
            "GitPython                     3.1.40\n",
            "graphql-core                  3.2.3\n",
            "greenlet                      3.0.2\n",
            "h11                           0.14.0\n",
            "hdbscan                       0.8.33\n",
            "idna                          3.4\n",
            "importlib-metadata            6.8.0\n",
            "ipykernel                     6.27.1\n",
            "ipython                       8.16.1\n",
            "ipywidgets                    8.1.1\n",
            "jedi                          0.19.1\n",
            "Jinja2                        3.1.2\n",
            "joblib                        1.3.2\n",
            "jsonpatch                     1.33\n",
            "jsonpointer                   2.4\n",
            "jupyter_client                8.5.0\n",
            "jupyter_core                  5.4.0\n",
            "jupyterlab-widgets            3.0.9\n",
            "langchain                     0.0.350\n",
            "langchain-community           0.0.3\n",
            "langchain-core                0.1.0\n",
            "langdetect                    1.0.9\n",
            "langsmith                     0.0.70\n",
            "llvmlite                      0.41.1\n",
            "loguru                        0.7.2\n",
            "lxml                          4.9.3\n",
            "MarkupSafe                    2.1.3\n",
            "marshmallow                   3.20.1\n",
            "matplotlib-inline             0.1.6\n",
            "multidict                     6.0.4\n",
            "mypy-extensions               1.0.0\n",
            "nest-asyncio                  1.5.8\n",
            "nltk                          3.8.1\n",
            "numba                         0.58.1\n",
            "numpy                         1.25.1\n",
            "openai                        0.28.1\n",
            "osmium                        3.6.0\n",
            "packaging                     23.2\n",
            "pandas                        2.0.3\n",
            "parso                         0.8.3\n",
            "pickleshare                   0.7.5\n",
            "pinecone-client               2.2.4\n",
            "pip                           23.1.2\n",
            "platformdirs                  3.11.0\n",
            "plotly                        5.15.0\n",
            "prompt-toolkit                3.0.39\n",
            "protobuf                      4.25.1\n",
            "psutil                        5.9.0\n",
            "pure-eval                     0.2.2\n",
            "pyarrow                       14.0.1\n",
            "pydantic                      2.5.2\n",
            "pydantic_core                 2.14.5\n",
            "Pygments                      2.16.1\n",
            "pynndescent                   0.5.11\n",
            "pypdf                         3.17.2\n",
            "python-dateutil               2.8.2\n",
            "python-dotenv                 1.0.0\n",
            "python-iso639                 2023.12.11\n",
            "python-magic                  0.4.27\n",
            "pytz                          2023.3\n",
            "pywin32                       305.1\n",
            "PyYAML                        6.0.1\n",
            "pyzmq                         23.2.1\n",
            "rapidfuzz                     3.5.2\n",
            "regex                         2023.10.3\n",
            "requests                      2.31.0\n",
            "scikit-learn                  1.2.2\n",
            "scipy                         1.11.4\n",
            "setuptools                    67.8.0\n",
            "six                           1.16.0\n",
            "smmap                         5.0.1\n",
            "sniffio                       1.3.0\n",
            "sortedcontainers              2.4.0\n",
            "soupsieve                     2.5\n",
            "SQLAlchemy                    2.0.23\n",
            "stack-data                    0.6.2\n",
            "starlette                     0.33.0\n",
            "strawberry-graphql            0.208.2\n",
            "tabulate                      0.9.0\n",
            "tenacity                      8.2.2\n",
            "threadpoolctl                 3.2.0\n",
            "tiktoken                      0.5.2\n",
            "tornado                       6.2\n",
            "tqdm                          4.66.1\n",
            "traitlets                     5.12.0\n",
            "typing_extensions             4.8.0\n",
            "typing-inspect                0.9.0\n",
            "tzdata                        2023.3\n",
            "umap-learn                    0.5.5\n",
            "unstructured                  0.11.2\n",
            "urllib3                       2.0.3\n",
            "uvicorn                       0.24.0.post1\n",
            "wcwidth                       0.2.8\n",
            "wheel                         0.38.4\n",
            "widgetsnbextension            4.0.9\n",
            "win32-setctime                1.1.0\n",
            "wrapt                         1.16.0\n",
            "yarl                          1.9.4\n",
            "zipp                          3.17.0\n"
          ]
        }
      ],
      "source": [
        "%pip install ipywidgets\n",
        "%pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wrap_text(cell):\n",
        "    return textwrap.fill(cell, width=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs8nuXRevhF8"
      },
      "source": [
        "**1.2. Open AI Login**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipykernel in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (6.27.1)\n",
            "Requirement already satisfied: comm>=0.1.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (0.1.4)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (1.6.7)\n",
            "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (8.16.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (8.5.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (5.4.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (1.5.8)\n",
            "Requirement already satisfied: packaging in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (5.9.0)\n",
            "Requirement already satisfied: pyzmq>=20 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (23.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (6.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipykernel) (5.12.0)\n",
            "Requirement already satisfied: backcall in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.39)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.16.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.2)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.11.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (305.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\abemd\\anaconda3\\envs\\stanford10\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXPUUQS3MmAs",
        "outputId": "3ac7c47c-bf53-48e8-bae4-12245ea10696"
      },
      "outputs": [],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval Tutorial\n",
        "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
        "    openai_api_key = getpass(prompt=\"🔑 Enter your OpenAI API key: \")\n",
        "openai.api_key = openai_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikfnAAhTvsbS"
      },
      "source": [
        "**1.3. Vector DB Logins**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHKhfe5EvmWN",
        "outputId": "a4dd11cd-f5e6-4fbd-a94e-f418ae1d21a8"
      },
      "outputs": [],
      "source": [
        "#pinecone login\n",
        "pinecone_api_key = getpass(prompt=\"🔑 Enter your Pinecone API key: \")\n",
        "pinecone_environment = getpass(prompt=\"set your Pinecone environment\")\n",
        "pinecone_index_name = getpass(prompt=\"set your Pinecone index name\")\n",
        "\n",
        "pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n",
        "\n",
        "#Saving environment Variables\n",
        "os.environ[\"YOUR_PINECONE_API_KEY\"] = pinecone_api_key\n",
        "os.environ[\"YOUR_PINECONE_INDEX_NAME\"]= pinecone_index_name\n",
        "os.environ[\"YOUR_PINECONE_ENVIRONMENT\"]= pinecone_environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODMvWw5Jvzo9"
      },
      "source": [
        "**1.4. Experimental Variables and Output Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NA6ubE0FMmxH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A subdirectory or file parq already exists.\n"
          ]
        }
      ],
      "source": [
        "#Experiment Variables: (You can change this to be more specific with Neptune and W&B and track there)\n",
        "os.environ[\"CHUNK_TYPES\"] = json.dumps([\"RecursiveCharacterTextSplitter\"])\n",
        "os.environ[\"CHUNK_SIZES\"] = json.dumps([512,400,300,256])\n",
        "os.environ[\"CHUNK_OVERLAPS\"] = json.dumps([20,10])\n",
        "\n",
        "%mkdir parq\n",
        "os.environ[\"OUTPUT_PARQUET_PATH\"]= r\"/parq\"\n",
        "os.environ[\"DOC_PATH\"]= r\"/test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn874pmEn7eU"
      },
      "source": [
        "**1.5. Set up Observability Tools**\n",
        "\n",
        "In this version we are setting up tracing for from Arize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "gExLvq7yn7p8",
        "outputId": "d4444e5c-41c9-4b58-97c7-3143eda69ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌍 To view the Phoenix app in your browser, visit http://127.0.0.1:6006/\n",
            "📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
            "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
          ]
        }
      ],
      "source": [
        "# Launch Phoenix to enable tracing\n",
        "session = px.launch_app()\n",
        "tracer = OpenInferenceTracer()\n",
        "LangChainInstrumentor(tracer).instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9xLaoDXKT27"
      },
      "source": [
        "**1.6. Clone Github Repo to help set up the right files for end users** - Not needed since this is already running on a local version? Need advice here..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8XaWVvrZjsJ",
        "outputId": "d73e919c-80de-4cc7-c89c-0f9faa3905a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abemd\\Documents\\Negotiator\\theNegotiator\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())\n",
        "\n",
        "@contextmanager\n",
        "def cwd(path):\n",
        "    oldpwd = os.getcwd()\n",
        "    os.chdir(path)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        os.chdir(oldpwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haozq08nKhf6",
        "outputId": "912ffbee-f4af-46ab-81b0-12ac16af6710"
      },
      "outputs": [],
      "source": [
        "# repo_url = \"https://github.com/abemdxb/theNegotiator.git\"\n",
        "# repo_path = \"/content/theNegotiator\"\n",
        "\n",
        "# !git clone \"https://github.com/abemdxb/theNegotiator.git\"\n",
        "\n",
        "# with cwd(repo_path):\n",
        "#   !git status\n",
        "\n",
        "# print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJAradYMne2n"
      },
      "source": [
        "# Step 2: Data Preparation\n",
        "\n",
        "**Background:** In this section, I am doing Data prep - specifically:\n",
        "1. Creating the contextual data in Vector Databases for RAG from three best practice textbooks/ articles for negotiations in sales, procurement and personal relationships.\n",
        "2. Creating the ⏰ 50 ⏰ example user queries that we can use to evaluate the model!\n",
        "\n",
        "**Instructions:** Run the following cells. Nothing else should be required. *Note: that for the user facing version of this notebook,  we will be importing the standard queries I developed earlier - this is to ensure that manual annotations done earlier are still usable. OpenAI's models can generate different queries over time.* *italicized text*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQBGQ9x1Hugk"
      },
      "source": [
        "**2.1. Create a new Pinecone Index and delete old one if it's already there**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o_kNe4ZAW-C",
        "outputId": "34de263e-ddff-42cb-fae3-5e7ccf13c050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index ready\n"
          ]
        }
      ],
      "source": [
        "# create the pinecone index  - sourced from https://github.com/pinecone-io/examples/blob/master/docs/quick-tour/hello-pinecone.ipynb\n",
        "\n",
        "#delete the old index to ensure we don't have duplicate chunks (needed if you have a free pinecone account- only one index allowed there)\n",
        "# if pinecone_index_name in pinecone.list_indexes():\n",
        "#     pinecone.delete_index(pinecone_index_name)\n",
        "#     pinecone.create_index(name=pinecone_index_name, dimension=1536, metric=\"cosine\")\n",
        "\n",
        "# wait for index to be ready before connecting\n",
        "while not pinecone.describe_index(pinecone_index_name).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"Index ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA60Lawgwbkm"
      },
      "source": [
        "**2.2. Download pdf and markdown versions of negotiation textbooks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\abemd\\\\Documents\\\\Negotiator\\\\theNegotiator'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2U-IfLeyttQ",
        "outputId": "867f1a05-62cc-4c5f-db01-70f2e5818ac5"
      },
      "outputs": [],
      "source": [
        "# print(os.getcwd())\n",
        "# !mkdir test\n",
        "# !curl -o test/paper1.pdf https://www.peaksellinginc.com/userfiles/25%20Most%20Difficult%20Negotiation%20Tactics.pdf\n",
        "# !curl -o test/paper2.pdf https://spada.uns.ac.id/pluginfile.php/238682/mod_resource/content/1/Roy%20J.%20Lewicki%2C%20Bruce%20Barry%2C%20David%20M.%20Saunders%20-%20Essentials%20of%20Negotiation-McGraw-Hill%20Education%20%282016%29.pdf\n",
        "# !curl -o test/paper3.pdf https://www.atlantis-press.com/article/125958466.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LcZgqw5ngf0",
        "outputId": "930b2d16-7001-4dc1-84be-247b5486ffd3"
      },
      "outputs": [],
      "source": [
        "#ignore until we find a better way to make markdown files\n",
        "\n",
        "# print(os.getcwd())\n",
        "# !mkdir testmd\n",
        "# !wget -O testmd/mdpaper1.md https://raw.githubusercontent.com/abemdxb/theNegotiator/main/test/25_Most_Difficult_Negotiation_Tactics.md\n",
        "# !wget -O testmd/mdpaper2.md https://raw.githubusercontent.com/abemdxb/theNegotiator/main/test/Essentials_of_Negotiation.md\n",
        "# !wget -O testmd/mdpaper3.md https://github.com/abemdxb/theNegotiator/blob/main/test/Negotiation_and_Romantic_Relationships.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynpgQuWVw1P0"
      },
      "source": [
        "**2.3. Run Python script to load the Pinecone Index with vectors**\n",
        "\n",
        "Note we use one namespace for each chunking strategy as specified by environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX5SwZrPwnVA",
        "outputId": "47f57354-9165-4adf-c60f-17cccb0980db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "docs_path=C:\\Users\\abemd\\Documents\\Negotiator\\theNegotiator\\test\n",
            "dpath=C:\\Users\\abemd\\Documents\\Negotiator\\theNegotiator\\test\n",
            "loader = <langchain_community.document_loaders.pdf.PyPDFDirectoryLoader object at 0x000001FA4C0F32E0>\n",
            "documents:[]\n",
            "stat dict: {'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {'': {'vector_count': 109}},\n",
            " 'total_vector_count': 109}\n",
            "chunking for namespace 0:RecursiveCharacterTextSplitter_512_20\n",
            "list of namespaces:{'': {'vector_count': 109}}\n",
            "RecursiveCharacterTextSplitter_512_20 namespace did not exist; new one created\n",
            "embeddings post build of pinecone:client=<class 'openai.api_resources.embedding.Embedding'> async_client=None model='text-embedding-ada-002' deployment='text-embedding-ada-002' openai_api_version='' openai_api_base=None openai_api_type='' openai_proxy='' embedding_ctx_length=8191 openai_api_key='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' openai_organization=None allowed_special=set() disallowed_special='all' chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None query_text_to_embedding={} document_text_to_embedding={}\n",
            "text_to_embedding: {}\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32m~\\Documents\\Negotiator\\theNegotiator\\multiple_chunk_strategy.py:281\u001b[0m\n\u001b[0;32m    278\u001b[0m build_pinecone_index(doc_iter, embeddings, pinecone_index_name, namespace)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m#Create df from embeddings\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m new_df \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_embedding_dataframe\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m#Create a diff_df that pulls only new rows from embeddings- not sure if build-index overwrites or not- not a opensource class\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32m~\\Documents\\Negotiator\\theNegotiator\\multiple_chunk_strategy.py:180\u001b[0m, in \u001b[0;36mOpenAIEmbeddingsWrapper.document_embedding_dataframe\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdocument_embedding_dataframe\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_text_to_embedding_map_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_text_to_embedding\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\Documents\\Negotiator\\theNegotiator\\multiple_chunk_strategy.py:187\u001b[0m, in \u001b[0;36mOpenAIEmbeddingsWrapper._convert_text_to_embedding_map_to_dataframe\u001b[1;34m(text_to_embedding)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_text_to_embedding_map_to_dataframe\u001b[39m(\n\u001b[0;32m    184\u001b[0m     text_to_embedding: Dict[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mfloat\u001b[39m]]\n\u001b[0;32m    185\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_to_embedding:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_to_embedding)\n\u001b[1;32m--> 187\u001b[0m     texts, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtext_to_embedding\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m    188\u001b[0m     embedding_arrays \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embeddings]\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(\n\u001b[0;32m    190\u001b[0m         {\n\u001b[0;32m    191\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: texts,\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m: embedding_arrays,\n\u001b[0;32m    193\u001b[0m         }\n\u001b[0;32m    194\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ],
      "source": [
        "# import subprocess\n",
        "# subprocess.run([\"python\",\"multiple_chunk_strategy.py\"])\n",
        "# %curl -o build_negotiation_pdf_index_langchain_pinecone.py https://raw.githubusercontent.com/abemdxb/theNegotiator/main/multiple_chunk_strategy.py\n",
        "%run multiple_chunk_strategy.py "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ckMjJBIT0j"
      },
      "source": [
        "**2.4 Create 50 synthetic queries/prompts via GPT-4, store them in a json**\n",
        "\n",
        "To Do: See if we can generate a better set of 50+ Q/A pairs from the other package that uses the chunks/ pdfs to create intelligent questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "YJn5L44Jgc9t",
        "outputId": "2fc6d4dc-0944-4252-f557-c025f3488e9e"
      },
      "outputs": [],
      "source": [
        "# Custom code to get the queries - note that I iterated on this prompt a few times and tried few shot techniques etc. but it reduced the variety of prompts being created which I thought was interesting for embeddings analysis.\n",
        "\n",
        "def  get_completion(prompt, model=\"gpt-4\", temperature=0):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "\n",
        "prompt = \"\"\" You are an assistant that only speaks in JSON format. Do not write normal text.\n",
        "\n",
        "Create 5 random scenarios where a target person is asking for help dealing with a challenging sales or procurement negotiation tactic used by their opponent.\n",
        "\n",
        "Ensure the scenario is in the form of a question from the perspective of a 'target person'.\n",
        "Vary whether the target person is in sales or procurement, the type of tactics/strategy their counterparty is using, and how specific they are being. Use no more that 20 words per question.\n",
        "\n",
        "Ensure the key used is 'text'.\"\"\"\n",
        "queries = get_completion(prompt)\n",
        "queries_json = json.loads(queries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHFGHI_ZNbyx"
      },
      "source": [
        "**2.5. [Do not uncomment -for Abe only] Save queries_json in the local Git clone of theNegotiator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoQrHsdzN0gE"
      },
      "outputs": [],
      "source": [
        "## insert code to save file to GitHub Repo\n",
        "file_name = f'/workingfiles/queries.json'\n",
        "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "with open(file_name, 'w') as file:\n",
        "    json.dump(queries_json, file)\n",
        "print(f'JSON file saved at: {file_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuuezcBWnNQQ"
      },
      "outputs": [],
      "source": [
        "# with cwd(repo_path):\n",
        "#   !git status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2qjHhb_N00c"
      },
      "source": [
        "**2.6. [Do not uncomment- works for Abe only]  Push Git Clone back to GitHub to update reference set of queries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38__xqcexTH5"
      },
      "outputs": [],
      "source": [
        "# github_token = getpass(\"Enter your GitHub personal access token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HwFDlr8ujOS"
      },
      "outputs": [],
      "source": [
        "# with cwd(repo_path):\n",
        "#   !git config --global user.email 'abemdxb@gmail.com'\n",
        "#   !git config --global user.name 'Abe Alappat'\n",
        "\n",
        "#   !git add -A\n",
        "#   !git commit -m \"add queries.json\"\n",
        "\n",
        "#   # Push to the remote repository using the token\n",
        "#   !git push \"https://abemdxb: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@github.com/abemdxb/theNegotiator.git\" --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0v3voOmx3UQ"
      },
      "source": [
        "**2.5 Inspect Query Dataframe containing User Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkOzsSNK5rlP"
      },
      "outputs": [],
      "source": [
        "#downloading Abe's established set of queries into a dataframe - leave uncommented as it is meant to\n",
        "query_df = pd.read_json(file_name)\n",
        "\n",
        "#Inspect the query\n",
        "query_df.head()\n",
        "query_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6B4iEAC47A5"
      },
      "outputs": [],
      "source": [
        "# # create df with new queries to see if they match old ones\n",
        "# query_df2 = pd.DataFrame.from_dict(queries_json)\n",
        "\n",
        "# query_df2.head()\n",
        "# query_df2.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQusO0dnif0a"
      },
      "source": [
        "**2.6. Create database_df for later use**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mkl_87YlGxIj",
        "outputId": "9616f4fe-1ac8-4f61-b5ee-8c8823b9ef79"
      },
      "outputs": [],
      "source": [
        "#Custom code\n",
        "\n",
        "#things to check- text_vector_y -> does this exist as part of the OpenAIEmbedding object?\n",
        "\n",
        "#load\n",
        "database_df = pd.read_parquet(\n",
        "    \"/parq/knowledge_db.pq\"\n",
        ")\n",
        "database_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7e7TKpqORCGC",
        "outputId": "a98b1d37-01d2-47dc-cc79-560e95819725"
      },
      "outputs": [],
      "source": [
        "database_df.describe()\n",
        "\n",
        "#things to check- why are there 2x the number of vectors in the database than in the chunked data? Ask Pinecone community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4l9pW5zin1B"
      },
      "source": [
        "# Step 3. Pipeline Creation\n",
        "\n",
        "**Background:** Create classes for use in creation of the Langchain Pipeline. To save time I re-used the classes from the Arize Tutorial\n",
        "\n",
        "**Instructions:** Run each cell. No additional inputs needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3BNtq2nnhDR"
      },
      "source": [
        "**3.1. Wrapper Class to create and store query and document embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlV3nsLhnhLf"
      },
      "outputs": [],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval Tutorial\n",
        "class OpenAIEmbeddingsWrapper(OpenAIEmbeddings):\n",
        "    \"\"\"\n",
        "    A wrapper around OpenAIEmbeddings that stores the query and document\n",
        "    embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    query_text_to_embedding: Dict[str, List[float]] = {}\n",
        "    document_text_to_embedding: Dict[str, List[float]] = {}\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        embedding = super().embed_query(text)\n",
        "        self.query_text_to_embedding[text] = embedding\n",
        "        return embedding\n",
        "\n",
        "    def embed_documents(self, texts: List[str], chunk_size: Optional[int] = 0) -> List[List[float]]:\n",
        "        embeddings = super().embed_documents(texts, chunk_size)\n",
        "        for text, embedding in zip(texts, embeddings):\n",
        "            self.document_text_to_embedding[text] = embedding\n",
        "        return embeddings\n",
        "\n",
        "    @property\n",
        "    def query_embedding_dataframe(self) -> pd.DataFrame:\n",
        "        return self._convert_text_to_embedding_map_to_dataframe(self.query_text_to_embedding)\n",
        "\n",
        "    @property\n",
        "    def document_embedding_dataframe(self) -> pd.DataFrame:\n",
        "        return self._convert_text_to_embedding_map_to_dataframe(self.document_text_to_embedding)\n",
        "\n",
        "    @staticmethod\n",
        "    def _convert_text_to_embedding_map_to_dataframe(\n",
        "        text_to_embedding: Dict[str, List[float]]\n",
        "    ) -> pd.DataFrame:\n",
        "        texts, embeddings = map(list, zip(*text_to_embedding.items()))\n",
        "        embedding_arrays = [np.array(embedding) for embedding in embeddings]\n",
        "        return pd.DataFrame.from_dict(\n",
        "            {\n",
        "                \"text\": texts,\n",
        "                \"text_vector\": embedding_arrays,\n",
        "            }\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuu0ZKpihGK"
      },
      "source": [
        "**3.2. Wrapper class to record retrieval data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl5tIiobOg-V"
      },
      "outputs": [],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval\n",
        "class PineconeWrapper(Pinecone):\n",
        "    query_text_to_document_score_tuples: Dict[str,List[Tuple[Document, float]]] = {}\n",
        "\n",
        "    def similarity_search_with_score(\n",
        "        self,\n",
        "        query: str,\n",
        "        k: int = 4,\n",
        "        filter: Optional[dict] = None,\n",
        "        namespace: Optional[str] = None,\n",
        "    ) -> List[Tuple[Document, float]]:\n",
        "        document_score_tuples = super().similarity_search_with_score(\n",
        "            query=query,\n",
        "            k=k,\n",
        "            filter=filter,\n",
        "            namespace=namespace,\n",
        "        )\n",
        "        #print(f\"query in pinecone={query}\")\n",
        "        self.query_text_to_document_score_tuples[query] = document_score_tuples\n",
        "        return document_score_tuples\n",
        "\n",
        "    @property\n",
        "    def retrieval_dataframe(self) -> pd.DataFrame:\n",
        "        query_texts = []\n",
        "        document_texts = []\n",
        "        retrieval_ranks = []\n",
        "        scores = []\n",
        "        for query_text, document_score_tuples in self.query_text_to_document_score_tuples.items():\n",
        "            for retrieval_rank, (document, score) in enumerate(document_score_tuples):\n",
        "                query_texts.append(query_text)\n",
        "                document_texts.append(document.page_content)\n",
        "                retrieval_ranks.append(retrieval_rank)\n",
        "                scores.append(score)\n",
        "        return pd.DataFrame.from_dict(\n",
        "            {\n",
        "                \"query_text\": query_texts,\n",
        "                \"document_text\": document_texts,\n",
        "                \"retrieval_rank\": retrieval_ranks,\n",
        "                \"score\": scores,\n",
        "            }\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7bG1zXMnhTu"
      },
      "source": [
        "# Step 4. (Ignore for now) Create a modifiable system prompt\n",
        "\n",
        "**Background:** Here I intended to create a system prompt which can guide the QA system's behavior.  This is for future experiments only\n",
        "\n",
        "**Instructions:** Run each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i76H9bxnhbw"
      },
      "outputs": [],
      "source": [
        "# Sources from https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "#rag_system_prompt = \"hub.pull(\"rlm/rag-prompt\")\"\n",
        "#print(rag_system_prompt)\n",
        "\n",
        "#from langchain.prompts import PromptTemplate\n",
        "#prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "#\n",
        "#{context}\n",
        "#\n",
        "#Question: {question}\n",
        "#Answer:\"\"\"\n",
        "#PROMPT = PromptTemplate(\n",
        "#    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "#)\n",
        "#\n",
        "#chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "#qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "\n",
        "#try the following https://www.aitidbits.ai/p/advanced-prompting for one blog post\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYa3PanInhlY"
      },
      "source": [
        "# Step 5. Test RAG Chain across multiple chunking strategies for a single test prompt\n",
        "\n",
        "**Background:** Adaptation of code from the Arize Tutorial. Adjusted to enable search within a specific set of namespaces and output the results.\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4GQpY_DQP9_"
      },
      "outputs": [],
      "source": [
        "#pull list of namespaces from python file- ensure there is an output somewhere with a json of namespaces?\n",
        "directory_path = os.environ.get(\"OUTPUT_PARQUET_PATH\")\n",
        "\n",
        "parquet_files = [file for file in os.listdir(directory_path) if file.endswith('.pq')]\n",
        "\n",
        "file_names_without_extension = [file.replace('.pq', '') for file in parquet_files]\n",
        "\n",
        "namespaces = pd.DataFrame({'File_Name': file_names_without_extension})\n",
        "\n",
        "namespaces.drop(namespaces[namespaces['File_Name'] == 'knowledge_db'].index, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksK6QudPaO51"
      },
      "outputs": [],
      "source": [
        "print(f\"namespaces from files:{namespaces}\")\n",
        "pindex=pinecone.Index(pineconce_index_name)\n",
        "stat_dict= p_index.describe_index_stats()\n",
        "list_of_namespaces=stat_dict[\"namespaces\"]\n",
        "print(f\"namespaces from the index itself:{list_of_namespaces}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9asFM0enht6"
      },
      "outputs": [],
      "source": [
        "# Adapted from Arize Pinecone Search and Retrieval\n",
        "embedding_model_name = \"text-embedding-ada-002\"\n",
        "num_retrieved_documents = 2\n",
        "chat_model_name = \"gpt-3.5-turbo\"\n",
        "embeddings = OpenAIEmbeddingsWrapper(model=embedding_model_name)\n",
        "llm = ChatOpenAI(model_name=chat_model_name)\n",
        "output_df = pd.DataFrame(columns=[\"namespace\", \"query_text\", \"query_embedding\", \"dimension\", \"response_text\"])\n",
        "\n",
        "for namespace in namespaces['File_Name']:\n",
        "    docsearch = PineconeWrapper.from_existing_index(\n",
        "        index_name = pinecone_index_name,\n",
        "        embedding = embeddings,\n",
        "        namespace = namespace  #New item\n",
        "    )\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=docsearch.as_retriever(search_kwargs={\"k\": num_retrieved_documents}),\n",
        "    )\n",
        "    query_text = \"Describe the hard ball negotiation tactic and provide an example. Use no more than 2 sentences.\"\n",
        "    response_text = chain.run(query_text)\n",
        "    retrievals_df = docsearch.retrieval_dataframe.tail(num_retrieved_documents)\n",
        "    contexts = retrievals_df[\"document_text\"].to_list()\n",
        "    scores = retrievals_df[\"score\"].to_list()\n",
        "    query_embedding = embeddings.query_embedding_dataframe[\"text_vector\"].iloc[-1]\n",
        "    dimension=len(query_embedding)\n",
        "\n",
        "    output_df.loc[len(output_df)] = [query_text, query_embedding, dimension, response_text, namespace]\n",
        "    for i, (context,score) in enumerate(zip(contexts, scores)):\n",
        "        output_df.at[len(output_df)-1,f\"Retrieved Context {i+1}\"]=contexts[i]\n",
        "        output_df.at[len(output_df)-1,f\"Retrieved Score {i+1}\"]=scores[i]\n",
        "\n",
        "    # Move the \"namespace\" column to the last position\n",
        "    output_df = output_df[[col for col in output_df.columns if col != \"namespace\"] + [\"namespace\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYPSSCZxNkJr"
      },
      "outputs": [],
      "source": [
        "output_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18m2qR8dGw8U"
      },
      "source": [
        "# 6. Running RAG for all queries and prepping data for evaluations\n",
        "\n",
        "**Background:** This section is where I build the data and normalize it.\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbUbc4BlgeD1"
      },
      "outputs": [],
      "source": [
        "#created this to be able to reset test_query_df without re-reunning everything above repeatedly.\n",
        "test_query_df = query_df.copy()\n",
        "test_query_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT0kYNB6J_3T"
      },
      "outputs": [],
      "source": [
        "#Custom code. Note the chain sometimes freezes after a few rows- and I used tracing to figure out that it is the LLM span and have requested clarity from OpenAI.\n",
        "for namespace in namespaces['File_Name']:\n",
        "    docsearch = PineconeWrapper.from_existing_index(\n",
        "        index_name = pinecone_index_name,\n",
        "        embedding = embeddings,\n",
        "        namespace = namespace  #New item\n",
        "    )\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=docsearch.as_retriever(search_kwargs={\"k\": num_retrieved_documents}),\n",
        "    )  #define the chain and docsearch:\n",
        "\n",
        "    for i, row in test_query_df.iterrows():\n",
        "        #print(f\"i={i}\")\n",
        "        query_text = row[\"text\"]\n",
        "        #print(f\"query={query_text}\")\n",
        "        response_text = chain.run(query_text)\n",
        "        #print(f\"response={response_text}\")\n",
        "        retrievals_df = docsearch.retrieval_dataframe.tail(num_retrieved_documents)\n",
        "        contexts = retrievals_df[\"document_text\"].to_list()\n",
        "        scores = retrievals_df[\"score\"].to_list()\n",
        "        query_embedding = embeddings.query_embedding_dataframe[\"text_vector\"].iloc[-1]\n",
        "\n",
        "        if \"text_vector\" not in test_query_df.columns:\n",
        "          test_query_df[\"text_vector\"] = None\n",
        "          test_query_df[\"text_vector\"] = test_query_df[\"text_vector\"].astype(object)\n",
        "        test_query_df.at[i,f\"text_vector\"] = query_embedding\n",
        "\n",
        "        test_query_df.at[i,f\"response\"] = response_text\n",
        "\n",
        "        for n, context in enumerate(contexts):\n",
        "            test_query_df.at[i,f\"Context_text_{n}\"] = context\n",
        "\n",
        "        for n, score in enumerate(scores):\n",
        "            test_query_df.at[i,f\"Context_similarity_{n}\"] = score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3tLp1BAJYuk"
      },
      "outputs": [],
      "source": [
        "#Again created this to avoid having to rerun other cells above\n",
        "query_with_response_df = test_query_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msr6W6M-XFwJ"
      },
      "outputs": [],
      "source": [
        "#Simulated an annotation with some random numbers - full annotation was paused for now to save time\n",
        "x = len(query_with_response_df)\n",
        "query_with_response_df['user_feedback'] = np.random.choice([-1, 1], size=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCQXenQN0nXl"
      },
      "source": [
        "**Center the embeddings in both the database and queries**\n",
        "\n",
        "Note: I did not know we could use df[\"column\"].mean() to average numpy nd arrays!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO0pL0wiEpF7"
      },
      "outputs": [],
      "source": [
        "## Adapted code to account for namespaces\n",
        "for namespace, group_df in database_df.groupby('namespace'):\n",
        "    database_group_centroid = group_df[\"text_vector_x\"].mean()\n",
        "    database_df.loc[group_df.index, \"centered_text_vector\"] = (\n",
        "        group_df[\"text_vector_x\"].apply(lambda x: x - database_group_centroid)\n",
        "    )\n",
        "for namespace, group_df in query_with_response_df.groupby('namespace'):\n",
        "    query_gp_centroid = group_df[\"text_vector\"].mean()\n",
        "    query_with_response_df.loc[group_df.index, \"centered_text_vector\"] = (\n",
        "        group_df[\"text_vector\"].apply(lambda x: x - query_gp_centroid)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEkP47eLCSI6"
      },
      "source": [
        "# 7. Run LLM Evaluations\n",
        "\n",
        "**Background:** This section is where the results are evaluated using LLMs\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te6eAV_lj18J"
      },
      "outputs": [],
      "source": [
        "#Sourced from the Arize Tutorial- just changed the df names\n",
        "EVALUATION_SYSTEM_MESSAGE = (\n",
        "    \"You will be given a query and a reference text. \"\n",
        "    \"You must determine whether the reference text contains an answer to the input query. \"\n",
        "    \"Your response must be binary (0 or 1) and \"\n",
        "    \"should not contain any text or characters aside from 0 or 1. \"\n",
        "    \"0 means that the reference text does not contain an answer to the query. \"\n",
        "    \"1 means the reference text contains an answer to the query.\"\n",
        ")\n",
        "QUERY_CONTEXT_PROMPT_TEMPLATE = \"\"\"# Query: {query}\n",
        "\n",
        "# Reference: {reference}\n",
        "\n",
        "# Binary: \"\"\"\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def evaluate_query_and_retrieved_context(query: str, context: str, model_name: str) -> str:\n",
        "    prompt = QUERY_CONTEXT_PROMPT_TEMPLATE.format(\n",
        "        query=query,\n",
        "        reference=context,\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MESSAGE},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        model=model_name,\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def evaluate_retrievals(\n",
        "    retrievals_data: Dict[str, str],\n",
        "    model_name: str,\n",
        ") -> List[str]:\n",
        "    responses = []\n",
        "    for query, retrieved_context in tqdm(retrievals_data.items()):\n",
        "        response = evaluate_query_and_retrieved_context(query, retrieved_context, model_name)\n",
        "        responses.append(response)\n",
        "    return responses\n",
        "\n",
        "\n",
        "def process_binary_responses(\n",
        "    binary_responses: List[str], binary_to_string_map: Dict[int, str]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Parse binary responses and convert to the desired format\n",
        "    converts them to the desired format. The binary_to_string_map parameter\n",
        "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
        "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
        "    \"\"\"\n",
        "    processed_responses = []\n",
        "    for binary_response in binary_responses:\n",
        "        try:\n",
        "            binary_value = int(binary_response.strip())\n",
        "            processed_response = binary_to_string_map[binary_value]\n",
        "        except (ValueError, KeyError):\n",
        "            processed_response = None\n",
        "        processed_responses.append(processed_response)\n",
        "    return processed_responses\n",
        "\n",
        "\n",
        "eval_query_df = query_with_response_df.copy()\n",
        "evaluation_model_name = \"gpt-4\"  # use GPT-4 if you have access\n",
        "for context_index in range(num_retrieved_documents):\n",
        "    retrievals_data = {\n",
        "        row[\"text\"]: row[f\"Context_text_{context_index}\"] for _, row in eval_query_df.iterrows()\n",
        "    }\n",
        "    raw_responses = evaluate_retrievals(retrievals_data, evaluation_model_name)\n",
        "    processed_responses = process_binary_responses(raw_responses, {0: \"irrelevant\", 1: \"relevant\"})\n",
        "    eval_query_df[f\"openai_relevance_{context_index}\"] = processed_responses\n",
        "eval_query_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mzas-kZeanr"
      },
      "source": [
        "## 8. Compute Precision @K and relevance, open the Pheonix Session, and save/load data for easy analysis\n",
        "\n",
        "**Background:** This section is where we calculate precision and launch the first pheonix session to be able to view the traces.\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNsDfelbeUKj"
      },
      "outputs": [],
      "source": [
        "#Sourced from the Arize Tutorial\n",
        "\n",
        "num_relevant_documents_array = np.zeros(len(eval_query_df))\n",
        "num_retrieved_documents = 2\n",
        "for retrieved_document_index in range(0, num_retrieved_documents):\n",
        "    num_retrieved_documents = retrieved_document_index + 1\n",
        "    num_relevant_documents_array += (\n",
        "        eval_query_df[f\"openai_relevance_{retrieved_document_index}\"]\n",
        "        .map(lambda x: int(x == \"relevant\"))\n",
        "        .to_numpy()\n",
        "    )\n",
        "    eval_query_df[f\"openai_precision@{num_retrieved_documents}\"] = pd.Series(\n",
        "        num_relevant_documents_array / num_retrieved_documents\n",
        "    )\n",
        "\n",
        "eval_query_df[\n",
        "    [\n",
        "        \"openai_relevance_0\",\n",
        "        \"openai_relevance_1\",\n",
        "        \"openai_precision@1\",\n",
        "        \"openai_precision@2\",\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmEHTLBL8UZw"
      },
      "outputs": [],
      "source": [
        "# add in recall @K, MaP, RAGAS, MRR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spgdYMR08dqx"
      },
      "outputs": [],
      "source": [
        "# add in calculation of p50,90,95,99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmC3pjfn8dgB"
      },
      "outputs": [],
      "source": [
        "# add in calculation of RPM and Char per min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0JOncdVrUy1"
      },
      "source": [
        "**Launch session url to analyze the trace data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i61ZfQy7rUXn"
      },
      "outputs": [],
      "source": [
        "#Custom code- Note I ran this cell in various locations to debug as I built the project up\n",
        "session.url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecu67G8I37Lh"
      },
      "source": [
        "**Save/Load data from my google drive**\n",
        "\n",
        " Note the actual file will be sent along with the other artifacts for the project. I did not set up a GCP account unfortunately!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iURul76037VF"
      },
      "outputs": [],
      "source": [
        "# Custom code - it will be commmented out before submission.\n",
        "\n",
        "spans_df=px.active_session().get_spans_dataframe('span_kind == \"RETRIEVER\"')\n",
        "outpt_path=os.getenv(\"OUTPUT_PARQUET_PATH\")\n",
        "file_path = f\"{outpt_path}/Arize/spans.pq\"\n",
        "\n",
        "spans_df.to_parquet(file_path, engine='pyarrow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbYlFEffUhV"
      },
      "source": [
        "# 9. Launch a new session of Pheonix to analyze embeddings data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opJaus1PeTr-"
      },
      "outputs": [],
      "source": [
        "#Code from Arize Tutorial\n",
        "\n",
        "query_schema = px.Schema(\n",
        "    prompt_column_names=px.EmbeddingColumnNames(\n",
        "        raw_data_column_name=\"text\",\n",
        "        vector_column_name=\"centered_text_vector\",\n",
        "    ),\n",
        "    response_column_names=\"response\",\n",
        "    tag_column_names=[\n",
        "        \"Context_text_0\",\n",
        "        \"Context_similarity_0\",\n",
        "        \"Context_text_1\",\n",
        "        \"Context_similarity_1\",\n",
        "        \"euclidean_distance_0\",\n",
        "        \"euclidean_distance_1\",\n",
        "        \"openai_relevance_0\",\n",
        "        \"openai_relevance_1\",\n",
        "        \"openai_precision@1\",\n",
        "        \"openai_precision@2\",\n",
        "        \"user_feedback\",\n",
        "    ],\n",
        ")\n",
        "database_schema = px.Schema(\n",
        "    document_column_names=px.EmbeddingColumnNames(\n",
        "        raw_data_column_name=\"text\",\n",
        "        vector_column_name=\"centered_text_vector\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DECaUGM1fZQj"
      },
      "outputs": [],
      "source": [
        "#Code from Arize Tutorial\n",
        "database_ds = px.Dataset(\n",
        "    dataframe=database_df,\n",
        "    schema=database_schema,\n",
        "    name=\"reference\",\n",
        ")\n",
        "query_ds = px.Dataset(\n",
        "    dataframe=eval_query_df,\n",
        "    schema=query_schema,\n",
        "    name=\"query\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmOUP7sqfibO"
      },
      "outputs": [],
      "source": [
        "#Code from Arize Tutorial\n",
        "session2 = px.launch_app(query_ds, corpus=database_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGsFC7BpQn8i"
      },
      "outputs": [],
      "source": [
        "file_path2 = f\"{outpt_path}/Arize/eval_query.pq\"\n",
        "\n",
        "eval_query_df.to_parquet(file_path2, engine='pyarrow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C93s6M5AhRPF"
      },
      "source": [
        "# 10. Calculate Average Precision @K,P50,P90,P95 from the data and set the baseline for results from Fixed Chunking Size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoUagyTDhQ4t"
      },
      "outputs": [],
      "source": [
        "#Average precision @K\n",
        "for namespace in namespaces:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YQZBAZShQW1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
