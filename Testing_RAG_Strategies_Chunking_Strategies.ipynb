{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl_eFVsjK3cA"
      },
      "source": [
        "# **The Negotiator: A Testbed for evaluating RAG strategies**\n",
        "Hello! Welcome to Abraham Alappat's personal project- a QA system to help people conduct negotiations for sales and procurement! In this and other notebooks, we will use a test-bed RAG QA product to test various strategies to optimize rag.\n",
        "\n",
        "In the longer run we will convert this test-bed into a productized ML pipeline that will be further iterated.\n",
        "\n",
        "**Goal for this particular notebook:** Test which chunking strategy helps optimize ML performance, latency and throughput for a set of standardized 50 questions.  \n",
        "\n",
        "**The Test-bed Product:** A Q/A tool that refers to best practices negotiation tactics for any given sales or procurement situation and does not contain contradictory information which is available in web-scale data. This notebook represents the MVP exploration of such a product and will be iterated on to test various aspects of .\n",
        "\n",
        "\n",
        "---------------------------------\n",
        "Note that this notebook follows the best tradition of software... re-use of code! I wanted a RAG based Q&A tool so adapted the Arize's Tutorials on Pinecone Search and Retrieval, Deep Learning's Prompt Engineering Course, and tracing.\n",
        "\n",
        "Things out of scope given the time for this project:\n",
        "1. Optimizing K, model temperature, model type, vector DB, system prompt, etc.\n",
        "\n",
        "*Code sourced from elsewhere will be marked as \"sourced from [source]\" in code comments*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bbAnVUIusb"
      },
      "source": [
        "# Step 1: Install Libraries and Dependences + Obtain Pinecone and OpenAI details\n",
        "**Background:** Here are we installing the usual libraries, packages and dependencies + collecting info to allow for downstream functions.\n",
        "\n",
        "**Intructions:** Run all cells and enter your OpenAI API Key, as well as your Pinecone API Key, environment and Index name to enable the full notebook to run! The Pinecone index should be set up in the same way as the Arize RAG tutorial.\n",
        "\n",
        "**1.1 Installations and Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install cython numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O8yi7Olm2Wu",
        "outputId": "4a89b550-3a98-4e21-e0d2-a461e9b86681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [40 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
            "      creating build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
            "      running build_ext\n",
            "      cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
            "      C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-build-env-8if_0mfo\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-install-kneknyio\\hdbscan_d676af328ae7469e8de353333c26c1f8\\hdbscan\\_hdbscan_tree.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
            "      C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-build-env-8if_0mfo\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-install-kneknyio\\hdbscan_d676af328ae7469e8de353333c26c1f8\\hdbscan\\_hdbscan_linkage.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
            "      C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-build-env-8if_0mfo\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-install-kneknyio\\hdbscan_d676af328ae7469e8de353333c26c1f8\\hdbscan\\_hdbscan_boruvka.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
            "      C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-build-env-8if_0mfo\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-install-kneknyio\\hdbscan_d676af328ae7469e8de353333c26c1f8\\hdbscan\\_hdbscan_reachability.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
            "      C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-build-env-8if_0mfo\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-install-kneknyio\\hdbscan_d676af328ae7469e8de353333c26c1f8\\hdbscan\\_prediction_utils.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
            "      C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-build-env-8if_0mfo\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\abemd\\AppData\\Local\\Temp\\pip-install-kneknyio\\hdbscan_d676af328ae7469e8de353333c26c1f8\\hdbscan\\dist_metrics.pxd\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      building 'hdbscan._hdbscan_tree' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for hdbscan\n",
            "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval Tutorial + Langchain Tutorial + DeepLearning Tutorials\n",
        "\n",
        "%pip install langchain openai==0.28.1 pinecone-client python-dotenv pypdf cohere tiktoken arize-phoenix unstructured fastparquet pyarrow GitPython tqdm  --quiet\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Base tools\n",
        "import os\n",
        "import textwrap\n",
        "from getpass import getpass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from typing_extensions import dataclass_transform\n",
        "from pandas.io import parquet\n",
        "# from google.colab import drive\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from contextlib import contextmanager\n",
        "from git import Repo\n",
        "\n",
        "#OpenAI and Langchain\n",
        "import openai\n",
        "import langchain\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain import hub\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "#Observational Tools and vector database\n",
        "import phoenix as px\n",
        "from phoenix.trace.langchain import OpenInferenceTracer, LangChainInstrumentor\n",
        "import pinecone\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                       Version\n",
            "----------------------------- --------\n",
            "asttokens                     2.4.0\n",
            "backcall                      0.2.0\n",
            "backports.functools-lru-cache 1.6.5\n",
            "certifi                       2023.5.7\n",
            "charset-normalizer            3.2.0\n",
            "colorama                      0.4.6\n",
            "comm                          0.1.4\n",
            "Cython                        3.0.6\n",
            "debugpy                       1.6.7\n",
            "decorator                     5.1.1\n",
            "exceptiongroup                1.1.3\n",
            "executing                     1.2.0\n",
            "idna                          3.4\n",
            "importlib-metadata            6.8.0\n",
            "ipykernel                     6.26.0\n",
            "ipython                       8.16.1\n",
            "jedi                          0.19.1\n",
            "joblib                        1.3.2\n",
            "jupyter_client                8.5.0\n",
            "jupyter_core                  5.4.0\n",
            "matplotlib-inline             0.1.6\n",
            "nest-asyncio                  1.5.8\n",
            "numpy                         1.25.1\n",
            "osmium                        3.6.0\n",
            "packaging                     23.1\n",
            "pandas                        2.0.3\n",
            "parso                         0.8.3\n",
            "pickleshare                   0.7.5\n",
            "pip                           23.1.2\n",
            "platformdirs                  3.11.0\n",
            "plotly                        5.15.0\n",
            "prompt-toolkit                3.0.39\n",
            "psutil                        5.9.0\n",
            "pure-eval                     0.2.2\n",
            "Pygments                      2.16.1\n",
            "python-dateutil               2.8.2\n",
            "pytz                          2023.3\n",
            "pywin32                       305.1\n",
            "pyzmq                         23.2.1\n",
            "requests                      2.31.0\n",
            "scikit-learn                  1.3.2\n",
            "scipy                         1.11.4\n",
            "setuptools                    67.8.0\n",
            "six                           1.16.0\n",
            "stack-data                    0.6.2\n",
            "tenacity                      8.2.2\n",
            "threadpoolctl                 3.2.0\n",
            "tornado                       6.2\n",
            "tqdm                          4.66.1\n",
            "traitlets                     5.12.0\n",
            "typing_extensions             4.8.0\n",
            "tzdata                        2023.3\n",
            "urllib3                       2.0.3\n",
            "wcwidth                       0.2.8\n",
            "wheel                         0.38.4\n",
            "zipp                          3.17.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs8nuXRevhF8"
      },
      "source": [
        "**1.2. Open AI Login**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXPUUQS3MmAs",
        "outputId": "3ac7c47c-bf53-48e8-bae4-12245ea10696"
      },
      "outputs": [],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval Tutorial\n",
        "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
        "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
        "openai.api_key = openai_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikfnAAhTvsbS"
      },
      "source": [
        "**1.3. Vector DB Logins**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHKhfe5EvmWN",
        "outputId": "a4dd11cd-f5e6-4fbd-a94e-f418ae1d21a8"
      },
      "outputs": [],
      "source": [
        "#pinecone login\n",
        "pinecone_api_key = getpass(prompt=\"🔑 Enter your Pinecone API key: \")\n",
        "pinecone_environment = getpass(prompt=\"set your Pinecone environment\")\n",
        "pinecone_index_name = getpass(prompt=\"set your Pinecone index name\")\n",
        "\n",
        "pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n",
        "\n",
        "#Saving environment Variables\n",
        "os.environ[\"YOUR_PINECONE_API_KEY\"] = pinecone_api_key\n",
        "os.environ[\"YOUR_PINECONE_INDEX_NAME\"]= pinecone_index_name\n",
        "os.environ[\"YOUR_PINECONE_ENVIRONMENT\"]= pinecone_environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODMvWw5Jvzo9"
      },
      "source": [
        "**1.4. Experimental Variables and Output Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA6ubE0FMmxH"
      },
      "outputs": [],
      "source": [
        "#Experiment Variables: (You can change this to be more specific with Neptune and W&B and track there)\n",
        "os.environ[\"CHUNK_TYPES\"] = json.dumps([\"RecursiveCharacterTextSplitter\"])\n",
        "os.environ[\"CHUNK_SIZES\"] = json.dumps([512,400,300,256])\n",
        "os.environ[\"CHUNK_OVERLAPS\"] = json.dumps([20,10])\n",
        "\n",
        "!mkdir parq\n",
        "os.environ[\"OUTPUT_PARQUET_PATH\"]= \"parq/\"\n",
        "os.environ[\"DOC_PATH\"]= \"test/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn874pmEn7eU"
      },
      "source": [
        "**1.5. Set up Observability Tools**\n",
        "\n",
        "In this version we are setting up tracing for from Arize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "gExLvq7yn7p8",
        "outputId": "d4444e5c-41c9-4b58-97c7-3143eda69ec0"
      },
      "outputs": [],
      "source": [
        "# Launch Phoenix to enable tracing\n",
        "session = px.launch_app()\n",
        "tracer = OpenInferenceTracer()\n",
        "LangChainInstrumentor(tracer).instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9xLaoDXKT27"
      },
      "source": [
        "**1.6. Clone Github Repo to help set up the right files for end users**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8XaWVvrZjsJ",
        "outputId": "d73e919c-80de-4cc7-c89c-0f9faa3905a5"
      },
      "outputs": [],
      "source": [
        "print(os.getcwd())\n",
        "\n",
        "@contextmanager\n",
        "def cwd(path):\n",
        "    oldpwd = os.getcwd()\n",
        "    os.chdir(path)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        os.chdir(oldpwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haozq08nKhf6",
        "outputId": "912ffbee-f4af-46ab-81b0-12ac16af6710"
      },
      "outputs": [],
      "source": [
        "repo_url = \"https://github.com/abemdxb/theNegotiator.git\"\n",
        "repo_path = \"/content/theNegotiator\"\n",
        "\n",
        "!git clone \"https://github.com/abemdxb/theNegotiator.git\"\n",
        "\n",
        "with cwd(repo_path):\n",
        "  !git status\n",
        "\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJAradYMne2n"
      },
      "source": [
        "# Step 2: Data Preparation\n",
        "\n",
        "**Background:** In this section, I am doing Data prep - specifically:\n",
        "1. Creating the contextual data in Vector Databases for RAG from three best practice textbooks/ articles for negotiations in sales, procurement and personal relationships.\n",
        "2. Creating the ⏰ 50 ⏰ example user queries that we can use to evaluate the model!\n",
        "\n",
        "**Instructions:** Run the following cells. Nothing else should be required. *Note: that for the user facing version of this notebook,  we will be importing the standard queries I developed earlier - this is to ensure that manual annotations done earlier are still usable. OpenAI's models can generate different queries over time.* *italicized text*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQBGQ9x1Hugk"
      },
      "source": [
        "**2.1. Create a new Pinecone Index and delete old one if it's already there**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o_kNe4ZAW-C",
        "outputId": "34de263e-ddff-42cb-fae3-5e7ccf13c050"
      },
      "outputs": [],
      "source": [
        "# create the pinecone index  - sourced from https://github.com/pinecone-io/examples/blob/master/docs/quick-tour/hello-pinecone.ipynb\n",
        "\n",
        "#delete the old index to ensure we don't have duplicate chunks (needed if you have a free pinecone account- only one index allowed there)\n",
        "if pinecone_index_name in pinecone.list_indexes():\n",
        "    pinecone.delete_index(pinecone_index_name)\n",
        "    pinecone.create_index(name=pinecone_index_name, dimension=1536, metric=\"cosine\")\n",
        "\n",
        "# wait for index to be ready before connecting\n",
        "while not pinecone.describe_index(pinecone_index_name).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"Index ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA60Lawgwbkm"
      },
      "source": [
        "**2.2. Download pdf and markdown versions of negotiation textbooks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2U-IfLeyttQ",
        "outputId": "867f1a05-62cc-4c5f-db01-70f2e5818ac5"
      },
      "outputs": [],
      "source": [
        "print(os.getcwd())\n",
        "!mkdir test\n",
        "!curl -o test/paper1.pdf https://www.peaksellinginc.com/userfiles/25%20Most%20Difficult%20Negotiation%20Tactics.pdf\n",
        "!curl -o test/paper2.pdf https://spada.uns.ac.id/pluginfile.php/238682/mod_resource/content/1/Roy%20J.%20Lewicki%2C%20Bruce%20Barry%2C%20David%20M.%20Saunders%20-%20Essentials%20of%20Negotiation-McGraw-Hill%20Education%20%282016%29.pdf\n",
        "!curl -o test/paper3.pdf https://www.atlantis-press.com/article/125958466.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LcZgqw5ngf0",
        "outputId": "930b2d16-7001-4dc1-84be-247b5486ffd3"
      },
      "outputs": [],
      "source": [
        "# print(os.getcwd())\n",
        "# !mkdir testmd\n",
        "# !wget -O testmd/mdpaper1.md https://raw.githubusercontent.com/abemdxb/theNegotiator/main/test/25_Most_Difficult_Negotiation_Tactics.md\n",
        "# !wget -O testmd/mdpaper2.md https://raw.githubusercontent.com/abemdxb/theNegotiator/main/test/Essentials_of_Negotiation.md\n",
        "# !wget -O testmd/mdpaper3.md https://github.com/abemdxb/theNegotiator/blob/main/test/Negotiation_and_Romantic_Relationships.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynpgQuWVw1P0"
      },
      "source": [
        "**2.3. Run Python script to load the Pinecone Index with vectors**\n",
        "\n",
        "Note we use one namespace for each chunking strategy as specified by environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX5SwZrPwnVA",
        "outputId": "47f57354-9165-4adf-c60f-17cccb0980db"
      },
      "outputs": [],
      "source": [
        "!curl -o build_negotiation_pdf_index_langchain_pinecone.py https://raw.githubusercontent.com/abemdxb/theNegotiator/main/multiple_chunk_strategy.py\n",
        "!python build_negotiation_pdf_index_langchain_pinecone.py --pinecone-api-key $YOUR_PINECONE_API_KEY --pinecone-index-name $YOUR_PINECONE_INDEX_NAME --pinecone-environment $YOUR_PINECONE_ENVIRONMENT --openai-api-key $OPENAI_API_KEY --output-parquet-path $OUTPUT_PARQUET_PATH  --docs-path $DOC_PATH --chunk-types \"$CHUNK_TYPES\" --chunk-sizes \"$CHUNK_SIZES\" --chunk-overlaps \"$CHUNK_OVERLAPS\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ckMjJBIT0j"
      },
      "source": [
        "**2.4 Create 50 synthetic queries/prompts via GPT-4, store them in a json**\n",
        "\n",
        "To Do: See if we can generate a better set of 50+ Q/A pairs from the other package that uses the chunks/ pdfs to create intelligent questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "YJn5L44Jgc9t",
        "outputId": "2fc6d4dc-0944-4252-f557-c025f3488e9e"
      },
      "outputs": [],
      "source": [
        "# Custom code to get the queries - note that I iterated on this prompt a few times and tried few shot techniques etc. but it reduced the variety of prompts being created which I thought was interesting for embeddings analysis.\n",
        "\n",
        "def  get_completion(prompt, model=\"gpt-4\", temperature=0):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "\n",
        "prompt = \"\"\" You are an assistant that only speaks in JSON format. Do not write normal text.\n",
        "\n",
        "Create 5 random scenarios where a target person is asking for help dealing with a challenging sales or procurement negotiation tactic used by their opponent.\n",
        "\n",
        "Ensure the scenario is in the form of a question from the perspective of a 'target person'.\n",
        "Vary whether the target person is in sales or procurement, the type of tactics/strategy their counterparty is using, and how specific they are being. Use no more that 20 words per question.\n",
        "\n",
        "Ensure the key used is 'text'.\"\"\"\n",
        "queries = get_completion(prompt)\n",
        "queries_json = json.loads(queries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHFGHI_ZNbyx"
      },
      "source": [
        "**2.5. [Do not uncomment -for Abe only] Save queries_json in the local Git clone of theNegotiator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoQrHsdzN0gE"
      },
      "outputs": [],
      "source": [
        "## insert code to save file to GitHub Repo\n",
        "file_name = f'{repo_path}/workingfiles/queries.json'\n",
        "os.environ['FILE_NAME'] = file_name\n",
        "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    json.dump(queries_json, file)\n",
        "\n",
        "print(f'JSON file saved at: {file_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuuezcBWnNQQ"
      },
      "outputs": [],
      "source": [
        "with cwd(repo_path):\n",
        "  !git status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2qjHhb_N00c"
      },
      "source": [
        "**2.6. [Do not uncomment- works for Abe only]  Push Git Clone back to GitHub to update reference set of queries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38__xqcexTH5"
      },
      "outputs": [],
      "source": [
        "github_token = getpass(\"Enter your GitHub personal access token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HwFDlr8ujOS"
      },
      "outputs": [],
      "source": [
        "with cwd(repo_path):\n",
        "  !git config --global user.email 'abemdxb@gmail.com'\n",
        "  !git config --global user.name 'Abe Alappat'\n",
        "\n",
        "  !git add -A\n",
        "  !git commit -m \"add queries.json\"\n",
        "\n",
        "  # Push to the remote repository using the token\n",
        "  !git push \"https://abemdxb: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@github.com/abemdxb/theNegotiator.git\" --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0v3voOmx3UQ"
      },
      "source": [
        "**2.5 Inspect Query Dataframe containing User Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkOzsSNK5rlP"
      },
      "outputs": [],
      "source": [
        "#downloading Abe's established set of queries into a dataframe - leave uncommented as it is meant to\n",
        "query_df = pd.read_json(file_name)\n",
        "\n",
        "#Inspect the query\n",
        "query_df.head()\n",
        "query_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6B4iEAC47A5"
      },
      "outputs": [],
      "source": [
        "# create df with new queries to see if they match old ones\n",
        "query_df2 = pd.DataFrame.from_dict(queries_json)\n",
        "\n",
        "query_df2.head()\n",
        "query_df2.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQusO0dnif0a"
      },
      "source": [
        "**2.6. Create database_df for later use**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mkl_87YlGxIj",
        "outputId": "9616f4fe-1ac8-4f61-b5ee-8c8823b9ef79"
      },
      "outputs": [],
      "source": [
        "#Custom code\n",
        "\n",
        "#things to check- text_vector_y -> does this exist as part of the OpenAIEmbedding object?\n",
        "\n",
        "#load\n",
        "database_df = pd.read_parquet(\n",
        "    \"/content/parq/knowledge_db.pq\"\n",
        ")\n",
        "database_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7e7TKpqORCGC",
        "outputId": "a98b1d37-01d2-47dc-cc79-560e95819725"
      },
      "outputs": [],
      "source": [
        "database_df.describe()\n",
        "\n",
        "#things to check- why are there 2x the number of vectors in the database than in the chunked data? Ask Pinecone community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4l9pW5zin1B"
      },
      "source": [
        "# Step 3. Pipeline Creation\n",
        "\n",
        "**Background:** Create classes for use in creation of the Langchain Pipeline. To save time I re-used the classes from the Arize Tutorial\n",
        "\n",
        "**Instructions:** Run each cell. No additional inputs needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3BNtq2nnhDR"
      },
      "source": [
        "**3.1. Wrapper Class to create and store query and document embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlV3nsLhnhLf"
      },
      "outputs": [],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval Tutorial\n",
        "class OpenAIEmbeddingsWrapper(OpenAIEmbeddings):\n",
        "    \"\"\"\n",
        "    A wrapper around OpenAIEmbeddings that stores the query and document\n",
        "    embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    query_text_to_embedding: Dict[str, List[float]] = {}\n",
        "    document_text_to_embedding: Dict[str, List[float]] = {}\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        embedding = super().embed_query(text)\n",
        "        self.query_text_to_embedding[text] = embedding\n",
        "        return embedding\n",
        "\n",
        "    def embed_documents(self, texts: List[str], chunk_size: Optional[int] = 0) -> List[List[float]]:\n",
        "        embeddings = super().embed_documents(texts, chunk_size)\n",
        "        for text, embedding in zip(texts, embeddings):\n",
        "            self.document_text_to_embedding[text] = embedding\n",
        "        return embeddings\n",
        "\n",
        "    @property\n",
        "    def query_embedding_dataframe(self) -> pd.DataFrame:\n",
        "        return self._convert_text_to_embedding_map_to_dataframe(self.query_text_to_embedding)\n",
        "\n",
        "    @property\n",
        "    def document_embedding_dataframe(self) -> pd.DataFrame:\n",
        "        return self._convert_text_to_embedding_map_to_dataframe(self.document_text_to_embedding)\n",
        "\n",
        "    @staticmethod\n",
        "    def _convert_text_to_embedding_map_to_dataframe(\n",
        "        text_to_embedding: Dict[str, List[float]]\n",
        "    ) -> pd.DataFrame:\n",
        "        texts, embeddings = map(list, zip(*text_to_embedding.items()))\n",
        "        embedding_arrays = [np.array(embedding) for embedding in embeddings]\n",
        "        return pd.DataFrame.from_dict(\n",
        "            {\n",
        "                \"text\": texts,\n",
        "                \"text_vector\": embedding_arrays,\n",
        "            }\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuu0ZKpihGK"
      },
      "source": [
        "**3.2. Wrapper class to record retrieval data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl5tIiobOg-V"
      },
      "outputs": [],
      "source": [
        "# Sourced from Arize Pinecone Search and Retrieval\n",
        "class PineconeWrapper(Pinecone):\n",
        "    query_text_to_document_score_tuples: Dict[str,List[Tuple[Document, float]]] = {}\n",
        "\n",
        "    def similarity_search_with_score(\n",
        "        self,\n",
        "        query: str,\n",
        "        k: int = 4,\n",
        "        filter: Optional[dict] = None,\n",
        "        namespace: Optional[str] = None,\n",
        "    ) -> List[Tuple[Document, float]]:\n",
        "        document_score_tuples = super().similarity_search_with_score(\n",
        "            query=query,\n",
        "            k=k,\n",
        "            filter=filter,\n",
        "            namespace=namespace,\n",
        "        )\n",
        "        #print(f\"query in pinecone={query}\")\n",
        "        self.query_text_to_document_score_tuples[query] = document_score_tuples\n",
        "        return document_score_tuples\n",
        "\n",
        "    @property\n",
        "    def retrieval_dataframe(self) -> pd.DataFrame:\n",
        "        query_texts = []\n",
        "        document_texts = []\n",
        "        retrieval_ranks = []\n",
        "        scores = []\n",
        "        for query_text, document_score_tuples in self.query_text_to_document_score_tuples.items():\n",
        "            for retrieval_rank, (document, score) in enumerate(document_score_tuples):\n",
        "                query_texts.append(query_text)\n",
        "                document_texts.append(document.page_content)\n",
        "                retrieval_ranks.append(retrieval_rank)\n",
        "                scores.append(score)\n",
        "        return pd.DataFrame.from_dict(\n",
        "            {\n",
        "                \"query_text\": query_texts,\n",
        "                \"document_text\": document_texts,\n",
        "                \"retrieval_rank\": retrieval_ranks,\n",
        "                \"score\": scores,\n",
        "            }\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7bG1zXMnhTu"
      },
      "source": [
        "# Step 4. (Ignore for now) Create a modifiable system prompt\n",
        "\n",
        "**Background:** Here I intended to create a system prompt which can guide the QA system's behavior.  This is for future experiments only\n",
        "\n",
        "**Instructions:** Run each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i76H9bxnhbw"
      },
      "outputs": [],
      "source": [
        "# Sources from https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "#rag_system_prompt = \"hub.pull(\"rlm/rag-prompt\")\"\n",
        "#print(rag_system_prompt)\n",
        "\n",
        "#from langchain.prompts import PromptTemplate\n",
        "#prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "#\n",
        "#{context}\n",
        "#\n",
        "#Question: {question}\n",
        "#Answer:\"\"\"\n",
        "#PROMPT = PromptTemplate(\n",
        "#    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "#)\n",
        "#\n",
        "#chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "#qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "\n",
        "#try the following https://www.aitidbits.ai/p/advanced-prompting for one blog post\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYa3PanInhlY"
      },
      "source": [
        "# Step 5. Test RAG Chain across multiple chunking strategies for a single test prompt\n",
        "\n",
        "**Background:** Adaptation of code from the Arize Tutorial. Adjusted to enable search within a specific set of namespaces and output the results.\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4GQpY_DQP9_"
      },
      "outputs": [],
      "source": [
        "#pull list of namespaces from python file- ensure there is an output somewhere with a json of namespaces?\n",
        "directory_path = os.environ.get(\"OUTPUT_PARQUET_PATH\")\n",
        "\n",
        "parquet_files = [file for file in os.listdir(directory_path) if file.endswith('.pq')]\n",
        "\n",
        "file_names_without_extension = [file.replace('.pq', '') for file in parquet_files]\n",
        "\n",
        "namespaces = pd.DataFrame({'File_Name': file_names_without_extension})\n",
        "\n",
        "namespaces.drop(namespaces[namespaces['File_Name'] == 'knowledge_db'].index, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksK6QudPaO51"
      },
      "outputs": [],
      "source": [
        "print(f\"namespaces from files:{namespaces}\")\n",
        "pindex=pinecone.Index(pineconce_index_name)\n",
        "stat_dict= p_index.describe_index_stats()\n",
        "list_of_namespaces=stat_dict[\"namespaces\"]\n",
        "print(f\"namespaces from the index itself:{list_of_namespaces}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9asFM0enht6"
      },
      "outputs": [],
      "source": [
        "# Adapted from Arize Pinecone Search and Retrieval\n",
        "embedding_model_name = \"text-embedding-ada-002\"\n",
        "num_retrieved_documents = 2\n",
        "chat_model_name = \"gpt-3.5-turbo\"\n",
        "embeddings = OpenAIEmbeddingsWrapper(model=embedding_model_name)\n",
        "llm = ChatOpenAI(model_name=chat_model_name)\n",
        "output_df = pd.DataFrame(columns=[\"namespace\", \"query_text\", \"query_embedding\", \"dimension\", \"response_text\"])\n",
        "\n",
        "\n",
        "\n",
        "for namespace in namespaces['File_Name']:\n",
        "    docsearch = PineconeWrapper.from_existing_index(\n",
        "        index_name = pinecone_index_name,\n",
        "        embedding = embeddings,\n",
        "        namespace = namespace  #New item\n",
        "    )\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=docsearch.as_retriever(search_kwargs={\"k\": num_retrieved_documents}),\n",
        "    )\n",
        "    query_text = \"Describe the hard ball negotiation tactic and provide an example. Use no more than 2 sentences.\"\n",
        "    response_text = chain.run(query_text)\n",
        "    retrievals_df = docsearch.retrieval_dataframe.tail(num_retrieved_documents)\n",
        "    contexts = retrievals_df[\"document_text\"].to_list()\n",
        "    scores = retrievals_df[\"score\"].to_list()\n",
        "    query_embedding = embeddings.query_embedding_dataframe[\"text_vector\"].iloc[-1]\n",
        "    dimension=len(query_embedding)\n",
        "\n",
        "    output_df.loc[len(output_df)] = [query_text, query_embedding, dimension, response_text, namespace]\n",
        "    for i, (context,score) in enumerate(zip(contexts, scores)):\n",
        "        output_df.at[len(output_df)-1,f\"Retrieved Context {i+1}\"]=contexts[i]\n",
        "        output_df.at[len(output_df)-1,f\"Retrieved Score {i+1}\"]=scores[i]\n",
        "\n",
        "    # Move the \"namespace\" column to the last position\n",
        "    output_df = output_df[[col for col in output_df.columns if col != \"namespace\"] + [\"namespace\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYPSSCZxNkJr"
      },
      "outputs": [],
      "source": [
        "output_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18m2qR8dGw8U"
      },
      "source": [
        "# 6. Running RAG for all queries and prepping data for evaluations\n",
        "\n",
        "**Background:** This section is where I build the data and normalize it.\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbUbc4BlgeD1"
      },
      "outputs": [],
      "source": [
        "#created this to be able to reset test_query_df without re-reunning everything above repeatedly.\n",
        "test_query_df = query_df.copy()\n",
        "test_query_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT0kYNB6J_3T"
      },
      "outputs": [],
      "source": [
        "#Custom code. Note the chain sometimes freezes after a few rows- and I used tracing to figure out that it is the LLM span and have requested clarity from OpenAI.\n",
        "for namespace in namespaces['File_Name']:\n",
        "    docsearch = PineconeWrapper.from_existing_index(\n",
        "        index_name = pinecone_index_name,\n",
        "        embedding = embeddings,\n",
        "        namespace = namespace  #New item\n",
        "    )\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=docsearch.as_retriever(search_kwargs={\"k\": num_retrieved_documents}),\n",
        "    )  #define the chain and docsearch:\n",
        "\n",
        "    for i, row in test_query_df.iterrows():\n",
        "        #print(f\"i={i}\")\n",
        "        query_text = row[\"text\"]\n",
        "        #print(f\"query={query_text}\")\n",
        "        response_text = chain.run(query_text)\n",
        "        #print(f\"response={response_text}\")\n",
        "        retrievals_df = docsearch.retrieval_dataframe.tail(num_retrieved_documents)\n",
        "        contexts = retrievals_df[\"document_text\"].to_list()\n",
        "        scores = retrievals_df[\"score\"].to_list()\n",
        "        query_embedding = embeddings.query_embedding_dataframe[\"text_vector\"].iloc[-1]\n",
        "\n",
        "        if \"text_vector\" not in test_query_df.columns:\n",
        "          test_query_df[\"text_vector\"] = None\n",
        "          test_query_df[\"text_vector\"] = test_query_df[\"text_vector\"].astype(object)\n",
        "        test_query_df.at[i,f\"text_vector\"] = query_embedding\n",
        "\n",
        "        test_query_df.at[i,f\"response\"] = response_text\n",
        "\n",
        "        for n, context in enumerate(contexts):\n",
        "            test_query_df.at[i,f\"Context_text_{n}\"] = context\n",
        "\n",
        "        for n, score in enumerate(scores):\n",
        "            test_query_df.at[i,f\"Context_similarity_{n}\"] = score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3tLp1BAJYuk"
      },
      "outputs": [],
      "source": [
        "#Again created this to avoid having to rerun other cells above\n",
        "query_with_response_df = test_query_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msr6W6M-XFwJ"
      },
      "outputs": [],
      "source": [
        "#Simulated an annotation with some random numbers - full annotation was paused for now to save time\n",
        "x = len(query_with_response_df)\n",
        "query_with_response_df['user_feedback'] = np.random.choice([-1, 1], size=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCQXenQN0nXl"
      },
      "source": [
        "**Center the embeddings in both the database and queries**\n",
        "\n",
        "Note: I did not know we could use df[\"column\"].mean() to average numpy nd arrays!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO0pL0wiEpF7"
      },
      "outputs": [],
      "source": [
        "## Adapted code to account for namespaces\n",
        "for namespace, group_df in database_df.groupby('namespace'):\n",
        "    database_group_centroid = group_df[\"text_vector_x\"].mean()\n",
        "    database_df.loc[group_df.index, \"centered_text_vector\"] = (\n",
        "        group_df[\"text_vector_x\"].apply(lambda x: x - database_group_centroid)\n",
        "    )\n",
        "for namespace, group_df in query_with_response_df.groupby('namespace'):\n",
        "    query_gp_centroid = group_df[\"text_vector\"].mean()\n",
        "    query_with_response_df.loc[group_df.index, \"centered_text_vector\"] = (\n",
        "        group_df[\"text_vector\"].apply(lambda x: x - query_gp_centroid)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEkP47eLCSI6"
      },
      "source": [
        "# 7. Run LLM Evaluations\n",
        "\n",
        "**Background:** This section is where the results are evaluated using LLMs\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te6eAV_lj18J"
      },
      "outputs": [],
      "source": [
        "#Sourced from the Arize Tutorial- just changed the df names\n",
        "EVALUATION_SYSTEM_MESSAGE = (\n",
        "    \"You will be given a query and a reference text. \"\n",
        "    \"You must determine whether the reference text contains an answer to the input query. \"\n",
        "    \"Your response must be binary (0 or 1) and \"\n",
        "    \"should not contain any text or characters aside from 0 or 1. \"\n",
        "    \"0 means that the reference text does not contain an answer to the query. \"\n",
        "    \"1 means the reference text contains an answer to the query.\"\n",
        ")\n",
        "QUERY_CONTEXT_PROMPT_TEMPLATE = \"\"\"# Query: {query}\n",
        "\n",
        "# Reference: {reference}\n",
        "\n",
        "# Binary: \"\"\"\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def evaluate_query_and_retrieved_context(query: str, context: str, model_name: str) -> str:\n",
        "    prompt = QUERY_CONTEXT_PROMPT_TEMPLATE.format(\n",
        "        query=query,\n",
        "        reference=context,\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MESSAGE},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        model=model_name,\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def evaluate_retrievals(\n",
        "    retrievals_data: Dict[str, str],\n",
        "    model_name: str,\n",
        ") -> List[str]:\n",
        "    responses = []\n",
        "    for query, retrieved_context in tqdm(retrievals_data.items()):\n",
        "        response = evaluate_query_and_retrieved_context(query, retrieved_context, model_name)\n",
        "        responses.append(response)\n",
        "    return responses\n",
        "\n",
        "\n",
        "def process_binary_responses(\n",
        "    binary_responses: List[str], binary_to_string_map: Dict[int, str]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Parse binary responses and convert to the desired format\n",
        "    converts them to the desired format. The binary_to_string_map parameter\n",
        "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
        "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
        "    \"\"\"\n",
        "    processed_responses = []\n",
        "    for binary_response in binary_responses:\n",
        "        try:\n",
        "            binary_value = int(binary_response.strip())\n",
        "            processed_response = binary_to_string_map[binary_value]\n",
        "        except (ValueError, KeyError):\n",
        "            processed_response = None\n",
        "        processed_responses.append(processed_response)\n",
        "    return processed_responses\n",
        "\n",
        "\n",
        "eval_query_df = query_with_response_df.copy()\n",
        "evaluation_model_name = \"gpt-4\"  # use GPT-4 if you have access\n",
        "for context_index in range(num_retrieved_documents):\n",
        "    retrievals_data = {\n",
        "        row[\"text\"]: row[f\"Context_text_{context_index}\"] for _, row in eval_query_df.iterrows()\n",
        "    }\n",
        "    raw_responses = evaluate_retrievals(retrievals_data, evaluation_model_name)\n",
        "    processed_responses = process_binary_responses(raw_responses, {0: \"irrelevant\", 1: \"relevant\"})\n",
        "    eval_query_df[f\"openai_relevance_{context_index}\"] = processed_responses\n",
        "eval_query_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mzas-kZeanr"
      },
      "source": [
        "## 8. Compute Precision @K and relevance, open the Pheonix Session, and save/load data for easy analysis\n",
        "\n",
        "**Background:** This section is where we calculate precision and launch the first pheonix session to be able to view the traces.\n",
        "\n",
        "**Instructions:** Run each cell below. No need for any user input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNsDfelbeUKj"
      },
      "outputs": [],
      "source": [
        "#Sourced from the Arize Tutorial\n",
        "\n",
        "#what is this? feed into ChatGPT and comment - seems like this is calculating precision @K?\n",
        "\n",
        "num_relevant_documents_array = np.zeros(len(eval_query_df))\n",
        "num_retrieved_documents = 2\n",
        "for retrieved_document_index in range(0, num_retrieved_documents):\n",
        "    num_retrieved_documents = retrieved_document_index + 1\n",
        "    num_relevant_documents_array += (\n",
        "        eval_query_df[f\"openai_relevance_{retrieved_document_index}\"]\n",
        "        .map(lambda x: int(x == \"relevant\"))\n",
        "        .to_numpy()\n",
        "    )\n",
        "    eval_query_df[f\"openai_precision@{num_retrieved_documents}\"] = pd.Series(\n",
        "        num_relevant_documents_array / num_retrieved_documents\n",
        "    )\n",
        "\n",
        "eval_query_df[\n",
        "    [\n",
        "        \"openai_relevance_0\",\n",
        "        \"openai_relevance_1\",\n",
        "        \"openai_precision@1\",\n",
        "        \"openai_precision@2\",\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmEHTLBL8UZw"
      },
      "outputs": [],
      "source": [
        "# add in recall @K, MaP, RAGAS, MRR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spgdYMR08dqx"
      },
      "outputs": [],
      "source": [
        "# add in calculation of p50,90,95,99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmC3pjfn8dgB"
      },
      "outputs": [],
      "source": [
        "# add in calculation of RPM and Char per min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0JOncdVrUy1"
      },
      "source": [
        "**Launch session url to analyze the trace data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i61ZfQy7rUXn"
      },
      "outputs": [],
      "source": [
        "#Custom code- Note I ran this cell in various locations to debug as I built the project up\n",
        "session.url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecu67G8I37Lh"
      },
      "source": [
        "**Save/Load data from my google drive**\n",
        "\n",
        " Note the actual file will be sent along with the other artifacts for the project. I did not set up a GCP account unfortunately!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iURul76037VF"
      },
      "outputs": [],
      "source": [
        "# Custom code - it will be commmented out before submission.\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "spans_df=px.active_session().get_spans_dataframe('span_kind == \"RETRIEVER\"')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Arize/spans.pq\"\n",
        "\n",
        "spans_df.to_parquet(file_path, engine='pyarrow')\n",
        "\n",
        "!ls '/content/drive/MyDrive/Arize/'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbYlFEffUhV"
      },
      "source": [
        "# 9. Launch a new session of Pheonix to analyze embeddings data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opJaus1PeTr-"
      },
      "outputs": [],
      "source": [
        "#Code from Arize Tutorial\n",
        "\n",
        "query_schema = px.Schema(\n",
        "    prompt_column_names=px.EmbeddingColumnNames(\n",
        "        raw_data_column_name=\"text\",\n",
        "        vector_column_name=\"centered_text_vector\",\n",
        "    ),\n",
        "    response_column_names=\"response\",\n",
        "    tag_column_names=[\n",
        "        \"Context_text_0\",\n",
        "        \"Context_similarity_0\",\n",
        "        \"Context_text_1\",\n",
        "        \"Context_similarity_1\",\n",
        "        \"euclidean_distance_0\",\n",
        "        \"euclidean_distance_1\",\n",
        "        \"openai_relevance_0\",\n",
        "        \"openai_relevance_1\",\n",
        "        \"openai_precision@1\",\n",
        "        \"openai_precision@2\",\n",
        "        \"user_feedback\",\n",
        "    ],\n",
        ")\n",
        "database_schema = px.Schema(\n",
        "    document_column_names=px.EmbeddingColumnNames(\n",
        "        raw_data_column_name=\"text\",\n",
        "        vector_column_name=\"centered_text_vector\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DECaUGM1fZQj"
      },
      "outputs": [],
      "source": [
        "#Code from Arize Tutorial\n",
        "database_ds = px.Dataset(\n",
        "    dataframe=database_df,\n",
        "    schema=database_schema,\n",
        "    name=\"reference\",\n",
        ")\n",
        "query_ds = px.Dataset(\n",
        "    dataframe=eval_query_df,\n",
        "    schema=query_schema,\n",
        "    name=\"query\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmOUP7sqfibO"
      },
      "outputs": [],
      "source": [
        "#Code from Arize Tutorial\n",
        "session2 = px.launch_app(query_ds, corpus=database_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGsFC7BpQn8i"
      },
      "outputs": [],
      "source": [
        "file_path2 = \"/content/drive/MyDrive/Arize/eval_query.pq\"\n",
        "\n",
        "eval_query_df.to_parquet(file_path2, engine='pyarrow')\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "source_path = '/content/parq/knowledge_db.pq'\n",
        "destination_path = '/content/drive/MyDrive/Arize/knowledge_db.pq'\n",
        "\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "\n",
        "\n",
        "!ls '/content/drive/MyDrive/Arize/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C93s6M5AhRPF"
      },
      "source": [
        "# 10. Calculate Average Precision @K,P50,P90,P95 from the data and set the baseline for results from Fixed Chunking Size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoUagyTDhQ4t"
      },
      "outputs": [],
      "source": [
        "#Average precision @K\n",
        "for namespace in namespaces:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YQZBAZShQW1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
